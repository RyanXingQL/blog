# 论文阅读

| 年份 | 标题 | 关键词 | 主要贡献 | 批注 |
| :- | :- | :- | :- | :- |
| 2022 | [Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models](https://arxiv.org/abs/2012.01988) | 多模型集群 | 本文的级联不是 boosting，而是逐步 ensemble；但效果完爆单模型和 ensemble |
| 2022 | [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) | Transformer - 卷积 | 探究 transformer 成功案例，将它们的 trick 应用于卷积，在 high-level visions 上取得了显著效果 | 在增强上试了试，效果不好 |
| 2022 | [VRT: A Video Restoration Transformer](https://arxiv.org/abs/2201.12288) | 视频恢复 - Transformer - 交互注意力 | 为视频帧设计了两两之间的 mutual attention 机制 | 显存要求高，无法同时处理较多帧，因而在长视频上无法超过 BasicVSR++ |
| 2021 | [SwinIR: Image Restoration Using Swin Transformer](https://arxiv.org/abs/2108.10257) | Transformer - 图像恢复 | 基于 swin transformer layer，堆叠，加残差，得到 residual swin transformer block（RSTB）；再堆叠 RSTB，加残差，得到整体网络 | 每一次卷积前，都要从 patch 组合为 image，即 unembed |
| 2021 | [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) | 自注意力 | 通过分辨率多层级、固定同一 window 内的 patch 数量，解决 transformer 计算量随整图分辨率平方增长的问题；通过 window 交错选取，实现不同 window 的信息交互；通过 cyclic shift，进一步减少计算量 |
| 2021 | [Video Super-Resolution Transformer](https://arxiv.org/abs/2106.06847) | Transformer - 视频超分辨率 | 把每一帧视为图像中的一个 patch，计算互相关 |
| 2021 | [Early Convolutions Help Transformers See Better](https://arxiv.org/abs/2106.14881) | Transformer - 卷积 | ViT 比传统卷积难训练；换掉其类似超大卷积的取块操作，转而使用多个小卷积，结果 ViT 精度更好 |
| 2021 | [Unsupervised Degradation Representation Learning for Blind Super-Resolution](https://arxiv.org/abs/2104.00416) | 图像超分辨率 | 基于 MoCov2，对图像的失真程度进行学习，得到的特征图输入后面的增强网络 | 具体而言，MoCov2 的输入会控制增强网络中可分离卷积的权重 |
| 2021 | [BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment](https://arxiv.org/abs/2104.13371) | 视频超分辨率 | 基于 flow 的 DCN：作者在之前的工作中研究过 DCN，因此在这篇文章中继续改进 DCN；在 AAAI 那篇工作中的发现和我之前发现很像，DCN 和光流的输出差异很小。因此把光流作为 DCN 的基础，DCN 学残差，是一种很好的缓解dcn训练难度的方法 | 这种研究的延续性非常好 |
| 2020 | [BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond](https://arxiv.org/abs/2012.02181) | 视频超分辨率 | 用 CNN 搭建的双向 RNN 结构 | ICON 引入关键帧和双向层交互。关键帧是用来稳定 RNN、减少误差传播的 |
| 2020 | [Understanding Deformable Alignment in Video Super-Resolution](https://arxiv.org/abs/2009.07265) | 视频超分辨率 | 把 DCN 拆解为两步，第一步获得 9 个通道，第二步 1x1 卷积融合。flow 本质上只在做第一步，且只有 1 个通道；因此性能有限；9 个通道差异越大，DCN 性能越好；顺便提出一个基于 flow 的稳定 loss |
| 2020 | [Invertible Image Rescaling](https://arxiv.org/abs/2005.05650) | 图像放缩 | | 实验过程中要施加两个 loss：重建（逆向）loss 和正向 loss；例如图像放缩，loss1 是复原的 hr 和原始 hr 比，loss2 是由原始 hr 产生的 lr 和真实 lr 比；实验发现，loss2 不能太大，否则 loss1 小不了。本文中，loss2 的权重设得特别小，即 lr 只要是看着好看就行，不需要 psnr 很高。因此，INN 不适用于只需要 lr 的过程，例如 SR 和增强 |
| 2020 | [HiFaceGAN: Face Renovation via Collaborative Suppression and Replenishment](https://arxiv.org/abs/2005.05005) | 人脸恢复 | 利用了 pix2pixHD，在 UNet 的解码端，基于编码端提供的信息，用 SPADE 做注意力建模 |
| 2020 | [Spatio-Temporal Deformable Convolution for Compressed Video Quality Enhancement](https://ojs.aaai.org//index.php/AAAI/article/view/6697) | 视频去压缩失真 | 用 DCN 在特征上做对齐 |
| 2020 | [Early Exit or Not: Resource-Efficient Blind Quality Enhancement for Compressed Images](https://arxiv.org/abs/2006.16581) | 图像去压缩失真 | 不同编码模式的增强可以共享推理结构和特征 | 问题：用 PSNR 随网络深度增加的斜率表征增强难度，用 QP 简单表征以训练；这种对增强难度的衡量及刻画很初浅。此外，虽然计算复杂度降低了，但时间复杂度上去了；因为使用了可分离卷积 |
| 2019 | [MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on Compressed Video](https://arxiv.org/abs/1902.09707) | 视频去压缩失真 | 相邻帧具有相关性；编码视频存在关键帧；先提取视频中的关键帧，然后充分利用关键帧，对非关键帧进行质量提升，有效缓和质量波动情况 | 问题：只考虑了 LDP 模式，其关键帧分布比较规律，因此提取也更容易。此外，iqa 采用 niqe，和目标 psnr 不一致 |
| 2019 | [How to Win Kaggle Competitions](https://www.youtube.com/watch?v=tsGGpe-onZI) | 深度学习竞赛 | 核心方法：让网络更好地收敛。从小图像开始，lr warmup，lr scheduler，data augmentation 逐步加上去，预训练。注意不要同时使用，要分阶段分开用，因为效果可能差不多，会叠加 |
| 2019 | [Path-Restore: Learning Network Path Selection for Image Restoration](https://arxiv.org/abs/1904.10343) | 图像恢复 | 让 CNN 的不同子路径处理不同失真程度的图像区域；基于强化学习，根据图像不同区域的不同增强效果，学一个 CNN 路径决策器 |
| 2019 | [Depth-Aware Video Frame Interpolation](https://arxiv.org/abs/1904.00830) | 插帧 | MEMC 中存在遮挡问题；浅层 object 不易被遮挡；因此生成 flow 时给浅层 flow 更大权值 | 很慢 |
| 2019 | [Deep Defocus Map Estimation using Domain Adaptation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Lee_Deep_Defocus_Map_Estimation_Using_Domain_Adaptation_CVPR_2019_paper.pdf) | 图像失焦预测 | DMENet：利用合成数据库和 deblur 数据库，实现 end2end 学习 | 用了两个数据库；第一个是合成数据库 SYNDOF，根据定义的 defocus map 生成图像；第二个是 CUHK blur detection 数据库，里面的图像都有很明显的前景和背景。二者互补，互相妥协 |
| 2018 | [ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/1809.00219) | 图像超分辨率 | 把对抗 loss 由判断绝对真伪改为判断相对真伪；组合网络参数而非图像，以实现保真和感知的 trade-off；把 perceptual loss 由激活后改为激活前测量 |
| 2018 | [COCO-Stuff: Thing and Stuff Classes in Context](https://openaccess.thecvf.com/content_cvpr_2018/papers/Caesar_COCO-Stuff_Thing_and_CVPR_2018_paper.pdf) | 图像分割 | 语义类别可以通过外形或纹理区分（通常没有确定的外形，例如青草，白云），前者称为 thing，后者称为 stuff；COCO 只把关键的外形标定了，而 COCO-Stuff 把背景也标定了 | 看图 1 |
| 2018 | [The Unreasonable Effectiveness of Deep Features as a Perceptual Metric](https://arxiv.org/abs/1801.03924) | 图像质量评估 | 基于 perceptual loss，进一步拟合与人类主观相似度评价之间的关系 | LPIPS（learned perceptual image patch similarity）：无论是无监督/有监督/自监督学习方法，VGG/AlexNet/SqueezeNet 网络架构，都是可行的；构建了一个大规模相似度评价数据库，包括传统失真和真实算法失真 |
| 2018 | [The 2018 PIRM Challenge on Perceptual Image Super-resolution](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Blau_2018_PIRM_Challenge_on_Perceptual_Image_Super-resolution_ECCVW_2018_paper.pdf) | 图像超分辨率 | 综合 NIQE 和 MA，得到了 PI 指标 | 由于 MA 慢，所以 PI 也慢 |
| 2017 | [GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://arxiv.org/abs/1706.08500) | GANs | 提出了 FID（Fréchet inception distance）指标，用来评价 GANs 的生成能力；此外，提出 TTUR 训练方法，并证明 TTUR 方法可以让鉴别器和生成器达到局部纳什均衡 |
| 2016 | [Fast defocus map estimation](https://ieeexplore.ieee.org/document/7533103) | 图像失焦预测 | 传统的 pixel-wise 预测太慢；本文在 edge 上预测 defocus；用 over-segmentation 得到像素块；计算像素块之间的相似性；再将 edge 的 defocus 推导至像素块及其相邻像素块，从而显著加速了 defocus 预测过程 | 很快，但不是很准；有些 defocus 预测反了 |
| 2016 | [Learning a No-Reference Quality Metric for Single-Image Super-Resolution](https://arxiv.org/abs/1612.05890) | 图像质量评估 | 在 PCA、DCT、小波域上同时评估超分图像的质量；得到特征以后，通过 regression forest，学习特征与主观评分之间的关系 | 库是用 180 张图像和 9 个超分算法生成的；共 50 个受试者；被后人称为 MA 算法；超级慢 |
| 2013 | [Salient Region Detection by UFO: Uniqueness, Focusness and Objectness](https://openaccess.thecvf.com/content_iccv_2013/papers/Jiang_Salient_Region_Detection_2013_ICCV_paper.pdf) | 显著性检测 | 前人大多用 uniqueness，例如像素或颜色上的独特性，来检测显著性；本文进一步结合 focusness 和 objectness | 所谓 objectness 是指某个像素附近存在一个完整物体的概率 |
| 2012 | [SLIC Superpixels Compared to State-of-the-Art Superpixel Methods](https://ieeexplore.ieee.org/document/6205760) | 图像分割 | 针对每一个像素，先得到一个五维向量（前 3 维是 CIELAB 色彩空间坐标，后 2 维是位置坐标）；然后对所有向量进行聚类；颜色和位置的相对权重不同，通过一个权重系数调整；此外还有一个系数控制超像素个数 | 这篇[博客](https://www.kawabangga.com/posts/1923)讲得蛮清楚的 |
| 2012 | [Making a “Completely Blind” Image Quality Analyzer](https://ieeexplore.ieee.org/document/6353522) | 图像质量评估 | 在 BRISQUE 的基础上，作者提出：不需要训练 SVM 拟合与人类主观评分的关系；只需要测量自然图像/失真图像的多维高斯分布之间的距离，即可用来衡量失真图像的失真程度 | NIQE |
| 2012 | [No-Reference Image Quality Assessment in the Spatial Domain](https://ieeexplore.ieee.org/document/6272356) | 图像质量评估 | 根据其他文献，图像中的归一化像素值 mean subtracted contrast normalized（MSCN）具有类似高斯分布特性。此外，相邻的 MSCN 之间也具有相关性，大致也呈高斯分布。因此，我们去学习 MSCN 分布及相邻 MSCN 点积分布的高斯参数，一共可获取 36 个可学习参数；利用 SVM 学习这 36 维参数和主观质量评分之间的关系 | BRISQUE：假设自然图像已经具有足够的统计量，可以用来评估图像质量；我们只需要统计这些统计量的偏移量，即可判断失真程度；不需要把图像变换到其他域（例如小波域，频域），也不需要针对特定失真进行设计 |
| 2004 | [Image quality assessment: from error visibility to structural similarity](https://ieeexplore.ieee.org/abstract/document/1284395) | 图像质量评估 | 作者认为，亮度、对比度和结构失真，是人类视觉系统对图像最敏感的 3 个信息；因此分别从图像块均值的相关性、标准差的相关性以及 pcc 相关系数来衡量两个图像块之间的相似性 | SSIM |
